{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDb Movies similarity from plot summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import and observe dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(5)\n",
    "\n",
    "movies_df = pd.read_csv('../dataset/movie_info.csv')\n",
    "\n",
    "# show the top 10 rows\n",
    "movies_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Tokenization and Stemming\n",
    "Tokenization is the process by which we break down articles into individual sentences or words, as needed. Besides the tokenization method provided by NLTK, we might have to perform additional filtration to remove tokens which are entirely numeric values or punctuation.\n",
    "\n",
    "Stemming is the process by which we bring down a word from its different forms to the root word. This helps us establish meaning to different forms of the same words without having to deal with each form separately.\n",
    "\n",
    "First, let us perform tokenization on a small extract from Mean Girls 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize a paragraph into sentences and store in sent_tokenized\n",
    "sent_tokenized = [sent for sent in nltk.sent_tokenize(\"\"\"\n",
    "                        The Plastics are back in the long-awaited follow-up to the smash hit Mean Girls - and now the clique is more fashionable, funny, and ferocious than ever.\n",
    "                        \"\"\")]\n",
    "\n",
    "# Word Tokenize first sentence from sent_tokenized, save as words_tokenized\n",
    "words_tokenized = [word for word in nltk.word_tokenize(sent_tokenized[0])]\n",
    "\n",
    "# Remove tokens that do not contain any letters from words_tokenized\n",
    "import re\n",
    "\n",
    "filtered = [word for word in words_tokenized if re.search('[a-zA-Z]', word)]\n",
    "\n",
    "# Display filtered words to observe words after tokenization\n",
    "filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the SnowballStemmer to perform stemming\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# Create an English language SnowballStemmer object\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# Print filtered to observe words without stemming\n",
    "print(\"Without stemming: \", filtered)\n",
    "\n",
    "# Stem the words from filtered and store in stemmed_words\n",
    "stemmed_words = [stemmer.stem(word) for word in filtered]\n",
    "\n",
    "# Print the stemmed_words to observe words after stemming\n",
    "print(\"After stemming:   \", stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now able to tokenize and stem sentences. But we may have to use the two functions repeatedly one after the other to handle a large amount of data, hence we can think of wrapping them in a function and passing the text to be tokenized and stemmed as the function argument. Then we can pass the new wrapping function, which shall perform both tokenizing and stemming instead of just tokenizing, as the tokenizer argument while creating the TF-IDF vector of the text.\n",
    "\n",
    "First, let us define a function to perform both stemming and tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_stem(text):\n",
    "    \n",
    "    # Tokenize by sentence, then by word\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    \n",
    "    # Filter out raw tokens to remove noise\n",
    "    filtered_tokens = [token for token in tokens if re.search('[a-zA-Z]', token)]\n",
    "    \n",
    "    # Stem the filtered_tokens\n",
    "    stems = [stemmer.stem(word) for word in filtered_tokens]\n",
    "    \n",
    "    return stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the function on the plot of Mean Girls 2 for example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_stemmed = tokenize_and_stem(\"The Plastics are back in the long-awaited follow-up to the smash hit Mean Girls - and now the clique is more fashionable, funny, and ferocious than ever.\")\n",
    "print(words_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "It gave us the same result, which means that the function is good to go.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create TfidfVectorizer\n",
    "Computers do not understand text. These are machines only capable of understanding numbers and performing numerical computation. Hence, we must convert our textual plot summaries to numbers for the computer to be able to extract meaning from them. One simple method of doing this would be to count all the occurrences of each word in the entire vocabulary and return the counts in a vector.\n",
    "\n",
    "Term Frequency-Inverse Document Frequency (TF-IDF) is one method which overcomes the shortcomings of CountVectorizer. The Term Frequency of a word is the measure of how often it appears in a document, while the Inverse Document Frequency is the parameter which reduces the importance of a word if it frequently appears in several documents.\n",
    "\n",
    "In simplest terms, TF-IDF recognizes words which are unique and important to any given document. Let's create one for our purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TfidfVectorizer to create TF-IDF vectors\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Instantiate TfidfVectorizer object with stopwords and tokenizer\n",
    "# parameters for efficient processing of text\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, max_features=200000,\n",
    "                                 min_df=2, stop_words='english',\n",
    "                                 use_idf=True, tokenizer=tokenize_and_stem,\n",
    "                                 ngram_range=(1,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Fit transform TfidfVectorizer\n",
    "Once we create a TF-IDF Vectorizer, the next step is to fit the text to it and then transform the text to produce the corresponding numeric form of the data which the computer will be able to understand and derive meaning from. To do this, we use the fit_transform() method of the TfidfVectorizer object.\n",
    "\n",
    "If we observe the TfidfVectorizer object we created, we come across a parameter stopwords. 'stopwords' are those words in a given text which do not contribute considerably towards the meaning of the sentence and are generally grammatical filler words.\n",
    "\n",
    "On setting the stopwords to 'english', we direct the vectorizer to drop all stopwords from a pre-defined list of English language stopwords present in the nltk module. Another parameter, ngram_range, defines the length of the ngrams to be formed while vectorizing the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform the tfidf_vectorizer with the \"plot\" of each movie\n",
    "# to create a vector representation of the plot summaries\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform([x for x in movies_df[\"plot\"]])\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. K-means clustering\n",
    "To determine how closely one movie is related to the other by the help of unsupervised learning, we can use clustering techniques. Clustering is the method of grouping together a number of items such that they exhibit similar properties. According to the measure of similarity desired, a given sample of items can have one or more clusters.\n",
    "\n",
    "   *  look for the elbow to determine the optimal number of clusters\n",
    "  \n",
    "   *  check the number of samples per group to confirm we have balanced samples accross k-means groups\n",
    "\n",
    "\n",
    "#### 1. look for the elbow to determine the optimal number of clusters\n",
    "\n",
    "Mini Batch K-means algorithmâ€˜s main idea is to use small random batches of data of a fixed size, so they can be stored in memory. Each iteration of a new random sample from the dataset is obtained and used to update the clusters and this is repeated until convergence. Each mini batch updates the clusters using a convex combination of the values of the prototypes and the data, applying a learning rate that decreases with the number of iterations. This learning rate is the inverse of the number of data assigned to a cluster during the process. As the number of iterations increases, the effect of new data is reduced, so convergence can be detected when no changes in the clusters occur in several consecutive iterations.\n",
    "\n",
    "Probably the most well known method, the elbow method, in which the sum of squares at each number of clusters is calculated and graphed, and the user looks for a change of slope from steep to shallow (an elbow) to determine the optimal number of clusters. This method is inexact, but still potentially helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "# init potential n_clusters\n",
    "n_clusters_list = list(range(1, 60, 1))\n",
    "# init scores\n",
    "scores = []\n",
    "# init models for cache\n",
    "kms = {}\n",
    "for n_clusters in n_clusters_list:\n",
    "    km = MiniBatchKMeans(\n",
    "        n_clusters=n_clusters,\n",
    "        random_state=99\n",
    "    ).fit(tfidf_matrix)\n",
    "    # save models\n",
    "    kms.update({n_clusters: km})\n",
    "    # save score\n",
    "    scores.append(-1 * km.score(tfidf_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look for elbow to determine optimal number of clusters\n",
    "pd.DataFrame({'scores': scores}, index=n_clusters_list).plot(\n",
    "    figsize=(16, 8),\n",
    "    title='K-Means Objective Score vs. Number of Clusters',\n",
    "    grid=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick optimal K-means Model\n",
    "n = 12\n",
    "km = kms[n]\n",
    "clusters = km.labels_.tolist()\n",
    "\n",
    "# Fit the k-means object with tfidf_matrix\n",
    "km.fit(tfidf_matrix)\n",
    "\n",
    "clusters = km.labels_.tolist()\n",
    "\n",
    "# create DataFrame df_clusters for clustering analysis\n",
    "data_clusters = {\n",
    "    'title': movies_df.title.values,\n",
    "    'plot': movies_df['plot'].values,\n",
    "    'run time': movies_df['run time /min'].values,\n",
    "    'votes': movies_df['number of votes'].values,\n",
    "    'rating':movies_df.rating.values,\n",
    "    'cluster': clusters\n",
    "}\n",
    "df_clusters = pd.DataFrame(\n",
    "    data_clusters,\n",
    "    index=[clusters],\n",
    "    columns=['title','plot','run time', 'votes','rating', 'cluster']\n",
    ")\n",
    "\n",
    "df_clusters.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. check the number of samples per group to confirm we have balanced samples accross k-means groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of movies included in each cluster:\")\n",
    "df_clusters['cluster'].value_counts().to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Calculate Movie Simialrity\n",
    "#### Use cosine similarity to calculate similarity of movie plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# get similarity matrix using tfidf matrix\n",
    "sim_matrix = cosine_similarity(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def movie_recommender_cosine_similarity(movie_title, K):\n",
    "    if (len(movies_df[movies_df['title']==movie_title])==0):\n",
    "        print(\"Sorry, we don't have this movie in our database. But we will take it into consideration in the future, thank you!\")\n",
    "    else:\n",
    "        movie_idx = movies_df[movies_df['title'] == movie_title].index[0]\n",
    "        return movies_df.loc[np.argsort(sim_matrix[movie_idx])[::-1][:K]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Again, let's use 'Mean Girls 2' as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_title = str(input(\"which movie you want to search? \"))\n",
    "K = int(input(\"How many most similarity movies you want to display? \"))\n",
    "\n",
    "movie_recommender_cosine_similarity(movie_title, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "What if we input some movie that doesn't exist in the dataset? say input \"I am not a movie\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_title = str(input(\"which movie you want to search? \"))\n",
    "K = int(input(\"How many most similarity movies you want to display? \"))\n",
    "\n",
    "movie_recommender_cosine_similarity(movie_title, K)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
