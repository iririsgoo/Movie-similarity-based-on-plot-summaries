{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDb Movies similarity from key words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import neccessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pandas.plotting import table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import movies table and set the index\n",
    "movies = pd.read_csv('../dataset/movie_info.csv')\n",
    "#movies['genre'] = [genre.split(\"|\") for genre in movies['genre']]\n",
    "#movies['key words'] = [genre.split(\"|\") for genre in movies['key words']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the time being, let's drop the plot column so that we can focus on calculating similarity that only uses keywords.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.drop(labels=['plot'], axis='columns', inplace=True)\n",
    "movies.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep dive into keywords\n",
    "The preprocessing steps for text are as follows:\n",
    "\n",
    "1. Lowercase the words\n",
    "2. Take .isalpha() words\n",
    "3. Remove Stop Words\n",
    "4. Lemmatize\n",
    "\n",
    "In our case, we will lowercase the words although it's not really necessary since they look all lowercase. It will be done for certainty.\n",
    "We will not take only alpha words because most of the keywords are compound words created with dashes (\"-\") and taking only alpha words would thus result in us discarding most of the words.\n",
    "We will remove stop words for completeness and safety although these are keywords so none should be stopwords.\n",
    "We will not lemmatize since doing do changes the meaning of certain keywords. For example, \"woods\" which indicates the forest, becomes \"wood\" the material. Or \"avengers\" becomes \"avenger\". In both cases, the first words have a meaning that is more than just the plural of the second words. So we will not take this step.\n",
    "\n",
    "Therefore in this case, we don't need to apply any of these steps to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.corpora.dictionary import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies['key words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_list = [keywords.split(\"|\") for keywords in movies['key words']]\n",
    "docs = []\n",
    "for keywords in keywords_list: \n",
    "    doc = []\n",
    "    for keyword in keywords:\n",
    "        doc.append(keyword)\n",
    "    docs = docs + doc\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create functions for making alpha, removing stop words, and lemmatizing\n",
    "def make_alpha(doc):\n",
    "    # Retain alphabetic words: alpha_only\n",
    "    alpha_only = [t for t in doc if t.isalpha()]    \n",
    "    return(alpha_only)\n",
    "def remove_stops(doc):\n",
    "    no_stops = [t for t in doc if t not in stopwords.words('english')]\n",
    "    return(no_stops)\n",
    "def lemmatize(doc):\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in doc]\n",
    "    return(lemmatized)\n",
    "def no_commas(doc):\n",
    "    no_commas = [t for t in doc if t!=',']\n",
    "    return(no_commas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code commented out below creates the processed_docs list which is what we use to find similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowercase_docs = [word_tokenize(doc.lower()) for doc in docs] #tokenize and lowercase\n",
    "\n",
    "import csv\n",
    "\n",
    "with open('lowercase.csv', 'w') as csvFile:\n",
    "    writer = csv.writer(csvFile)\n",
    "    writer.writerows(lowercase_docs)\n",
    "    csvFile.close()\n",
    "\n",
    "\n",
    "with open('lowercase.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    lowercase_docs = list(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowercase_and_no_stop_docs = [remove_stops(doc) for doc in lowercase_docs]\n",
    "\n",
    "with open('lowercase_and_no_stops.csv', 'w') as csvFile:\n",
    "    writer = csv.writer(csvFile)\n",
    "    writer.writerows(lowercase_and_no_stop_docs)\n",
    "    csvFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowercase_nostops_nocommas_docs = [no_commas(doc) for doc in lowercase_and_no_stop_docs]\n",
    "processed_docs = lowercase_nostops_nocommas_docs\n",
    "\n",
    "with open('processed_docs.csv', 'w') as csvFile:\n",
    "    writer = csv.writer(csvFile)\n",
    "    writer.writerows(processed_docs)\n",
    "    csvFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is all the relevent code for the model in one cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('processed_docs.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    processed_docs = list(reader)\n",
    "    processed_docs = processed_docs[0::2] # get rid of empty lists\n",
    "\n",
    "dictionary = Dictionary(processed_docs) # create a dictionary of words from our keywords\n",
    "\n",
    "corpus = [dictionary.doc2bow(doc) for doc in processed_docs] #create corpus where the corpus is a bag of words for each document\n",
    "\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "\n",
    "tfidf = TfidfModel(corpus) #create tfidf model of the corpus\n",
    "\n",
    "import gensim\n",
    "from gensim.similarities import Similarity\n",
    "from gensim.similarities import MatrixSimilarity\n",
    "\n",
    "# Create the similarity data structure. This is the most important part where we get the similarities between the movies.\n",
    "sims = MatrixSimilarity(tfidf[corpus], num_features=len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary(processed_docs) # create a dictionary of words from our keywords\n",
    "\n",
    "# Print out first 10 words:\n",
    "for i in range(len(dictionary))[0:10]:\n",
    "    print(i, dictionary[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create corpus where the corpus is a bag of words for each document\n",
    "corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the defaultdict: total_word_count\n",
    "# This dictionary contains every word ID and its corresponding number of times it appears in the corpus\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "total_word_count = defaultdict(int)\n",
    "for word_id, word_count in itertools.chain.from_iterable(corpus):\n",
    "    total_word_count[word_id] += word_count\n",
    "\n",
    "# Create a sorted list from the defaultdict: sorted_word_count\n",
    "sorted_word_count = sorted(total_word_count.items(), key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the top 20 words across all documents alongside the count\n",
    "for word_id, word_count in sorted_word_count[:20]:\n",
    "    print(dictionary.get(word_id), word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly a lot of christmas themed movies and relationships in these keywords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1st Model: Jaccard Similarity Based on Word Counts\n",
    "Jaccard similarity(df) : intersection of 2 sets divided by the union of those sets.\n",
    "\n",
    "The idea of this model:\n",
    "\n",
    "\\# of common keywords between two movies / # of unique keywords in the union of two movies’ keywords\n",
    "\n",
    "Then we rank the movies by their similarities and the user can query the top K results for each movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_jaccard_sim(str1, str2):\n",
    "    a = set(str1.split('|'))\n",
    "    b = set(str2.split('|'))\n",
    "    c = a.intersection(b)\n",
    "    return(float(len(c)) / (len(a) + len(b) - len(c)))\n",
    "\n",
    "def keyword_string(movie):\n",
    "    movie = movies[movies.title==movie]\n",
    "    keyword_string = movie['key words'].iloc[0]\n",
    "    \n",
    "    return(keyword_string)\n",
    "\n",
    "def get_jaccard_sim2(movie1, movie2):\n",
    "    keywords1 = keyword_string(movie1)\n",
    "    keywords2 = keyword_string(movie2)\n",
    "    \n",
    "    return(get_jaccard_sim(keywords1, keywords2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_recommender(movie_title, K=5):\n",
    "    if (len(movies[movies['title']==movie_title])==0):\n",
    "        print(\"Sorry, we don't have this movie in our database. But we will take it into consideration in the future, thank you!\")\n",
    "    else:\n",
    "        movie = movies[movies.title==movie_title]\n",
    "        keyword_string = movie['key words'].iloc[0]\n",
    "\n",
    "        jaccards = []\n",
    "        for movie in movies['key words']:\n",
    "            jaccards.append(get_jaccard_sim(keyword_string, movie))\n",
    "        jaccards = pd.Series(jaccards)\n",
    "        jaccards_index = jaccards.nlargest(K+1).index\n",
    "        matches = movies.loc[jaccards_index]\n",
    "        for match,score in zip(matches['title'][1:],jaccards[jaccards_index][1:]) :\n",
    "            print(match,score )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2nd Model: Cosine Similarity Based on Word Counts\n",
    "1. use CountVectorizer, Compute word counts for every movie’s keywords (word vectors)\n",
    "\n",
    "2. Use scikit-learn library, Compute cosine similarity between any word vectors\n",
    "\n",
    "Like the 1st model, then we rank the movies by their similarities and the user can query the top K results for each movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_cosine_sim(*strs):\n",
    "    vectors = [t for t in get_vectors1(*strs)]\n",
    "    return(cosine_similarity(vectors))\n",
    "\n",
    "def get_vectors1(*strs):\n",
    "    text = [t for t in strs]\n",
    "    vectorizer = CountVectorizer(text)\n",
    "    vectorizer.fit(text)\n",
    "    return(vectorizer.transform(text).toarray())\n",
    "\n",
    "def get_vectors2(text):\n",
    "    vectorizer = CountVectorizer(text)\n",
    "    X = vectorizer.fit_transform(text)\n",
    "    return(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_vectors2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3b9e3b616c7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_vectors2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmovies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'key words'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'get_vectors2' is not defined"
     ]
    }
   ],
   "source": [
    "vectors = get_vectors2(movies['key words'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_recommender(movie_title, K=5):\n",
    "    if (len(movies[movies['title']==movie_title])==0):\n",
    "        print(\"Sorry, we don't have this movie in our database. But we will take it into consideration in the future, thank you!\")\n",
    "    else:\n",
    "        movie_index = movies[movies.title == movie_title].index.values[0]\n",
    "\n",
    "        cosines = []\n",
    "        for i in range(len(vectors)):\n",
    "            vector_list = [vectors[movie_index], vectors[i]]\n",
    "            cosines.append(cosine_similarity(vector_list)[0,1])\n",
    "\n",
    "        cosines = pd.Series(cosines)\n",
    "        index = cosines.nlargest(K+1).index\n",
    "\n",
    "        matches = movies.loc[index]\n",
    "        for match,score in zip(matches['title'][1:],cosines[index][1:]):\n",
    "            print(match,score )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick comparison for a movie based on different models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use 'Mean Girls 2' as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_title = str(input(\"which movie you want to search? \"))\n",
    "K = int(input(\"How many most similarity movies you want to display? \"))\n",
    "\n",
    "jaccard_recommender(movie_title, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_title = str(input(\"which movie you want to search? \"))\n",
    "K = int(input(\"How many most similarity movies you want to display? \"))\n",
    "\n",
    "jaccard_recommender(movie_title, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Looks good. They all recommend similar appropriate movies for the same movie, with slight differences in recommendation.\n",
    "\n",
    "What if we input some movie that doesn't exist in the dataset? say input \"I am not a movie\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_title = str(input(\"which movie you want to search? \"))\n",
    "K = int(input(\"How many most similarity movies you want to display? \"))\n",
    "\n",
    "jaccard_recommender(movie_title, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_title = str(input(\"which movie you want to search? \"))\n",
    "K = int(input(\"How many most similarity movies you want to display? \"))\n",
    "\n",
    "jaccard_recommender(movie_title, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
